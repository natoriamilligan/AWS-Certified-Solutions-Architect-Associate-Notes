# S3
 - infinitely scaling storage
 ### Durability:
   - high durability (99.999999999% 11 9's) of objects accross multiple AZ
   - if you store 10,000,000 objects with S3, you can on avg expect to incur a loss of a single object once every 10,000 years
   - same for all storage classes
 ### Availability:
   - measures how readily avaiable a service is
   - varies depending on storage class
   - ex. S3 stand has 99.99% availabilty = not available for 53 minutes a year
 - Use Cases
  - backup and storage
  - disaster recovery
  - archive
  - hybrid cloud storage
  - Applicating hosting
  - media hosting
  - data lake & bid data anaylytics
  - software delivery
  - static websites

  ## Buckets
  - allows people to store objects (files) into buckets (directories)
  - buckets must have a globally unique name (accross all regons and all accounts)
  - defined at the region level
  - looks like a global service but buckets are created in a region
  - Nameing convention:
    - no uppercase
    - no underscore
    - 3-63 characters long
    - not an IP
    - must start with a lowercase letter or number
    - must NOT start with the prefix xn
    - must NOT end with the suffix s3alias
   
  ## Objects
  - objects have a key
  - the key is the Full path:
    - ex. s3://my-bucket/my-file.txt
  - the key is composed of the prefix + object name
    - ex. ex. s3://my-bucket/folder1/folder2/my-file.txt (folder1 and 2 are prefixes and the file is the object name)
  - S3 does not have a concept of directoris within buckets
  - keys are very long names that contain slashes (/)
  - object values are the content of the body:
    - max object size is 5TB (5000GB)
    - if uploading more than 5GB, must use the multi-part upload
  - objects can have metadata (list of key value pairs)
  - tags (unicode / value pair - up to 10). useful for security / lifecycle
  - Version ID (if versioning is enabled)

  ## Security
   - Encryption : encrypt objects in S3 using encryption keys
   - User-based
     - IAM policies : which API calls should be allowed for aspecific user from IAM
   - Resource-based
     - Bucket Policies (most common): bucket wide rules form the S3 console - allows cross account (allows a user to come in or a user from another account come in)
     - Object ACcess Control List : finer grain (can be disabled)
     - Bucket Access Control List : less common (can be disabled)
   - NOTE: an IAM principle can access an S3 object if:
     1. the user IAM permissions ALLOW it OR the resource policy ALLOWS it
     2. AND theres no explicit DENY
    
     ### Bucket Policies
      - jSON
        - Resources : tells the policy what buckets and objects this policy applies on
        - Effect: ALLOW/ Deny
        - Actions : Set of APIs you allow or deny
        - Principle : the account or user to apply the policy to
      - Use s# bucket policy to:
        - grant public access to the bucket
        - force objects to be encrypted at upload
        - grant access to another account (cross account)
       
      ### Bucket settings for Block Public Access
       - these setting were create to prevent data leaks
       - if you know that your bucket should never be public, leave these on
       - can be set at the account level

 ## Static Website Hosting
  - S3 can host static websites and have them accessible on the internet
  - the website URL will be (depening on the region):
    1. http://**bucket-name**.s3-website-**aws-region**.amazonaws.com OR
    2. http://**bucket-name**.s3-website.**aws-region**.amazonaws.com
   
 ## Versioning
  - you can version by enabled at bucket level
  - same key overwrite will change the new version
  - best practice to version your bucket
    - protexts afainst unintended delets (ability to restore a version)
    - easy rollback to previoud version
  - NOTES :
    - any file that is not versioned prior to enbaling versioning will have version null
    - suspending versioning does not delete the previoud versions
    - to do a version, you upload the updated file with the exact name and it will put it as a new version. Delete new version to go back a version
    - if you delete a file, it will create a delete marker and you will be able to go back and restore that deleted file. This only happens if versioning is enabled
   
  ## Replication (CRR & SRR)
   - settig up async replication between 2 buckets in different regions
   - must enable versioning in source and destination buckets
   - Cross-Regional Replication : two regions different
   - Same-Regional Replication : two regions same
   - buckets can be in different AWS accounts
   - must give proper IAM permissions to S3
   - after you enable replication, only new obejcts will be replicated
   - to replicate existing objects use S3 Batch Relication
     - replicates exising objects and objects that failed replication
   - for DELETE operations:
     - can replicate delete markers from cource to target (optional setting)
     - deletions with a version ID are not replicated (to avoid malicious deletes)
   -  there is no chaining of replication : if bucket 1 replicates into 2 and 2 replicates into 3, bucket 3 will not have bucket 1 files.
   - use cases:
     - CRR - compliance, lower latency access, replication accross accounts
     - SRR - log aggregation, live replication btw production and test accounts
   - you can enable delete markers to be replicted but permantent deletes are not replicated

## Storage Classes
 - make sure to look at the pricing chart!
 - you can move btw classes manually or using S3 Lifecycle configurations(under management tab)
 ### Amazon S3 Standard (General Purpose)
  - 99.99 availbiity
  - used to frequently accessed data
  -  low latency and high throughput
  -  sustain 2 concurrent facility features
  -  use cases : big data analytics, mobile and gaming apps, content distribution
 ### Amazon S3 Standard-Infrequent Access (IA)
  - for data that is less frequently acesed, but requires rapid access when needed
  - lower cost than S3 standard
  - 99.99% availability
  - use cases : disaster recovery, backups
 ### Amazon S3 One Zone-Infrequent Access
  - for data that is less frequently acesed, but requires rapid access when needed
  - lower cost than S3 standard
  - high durability in a single AZ only, data lost when AZ is destroyed
  - 99.5% availability
  - use cases: storing secondary backup copies of on-permise data, or data you can recreate
 ### Amazon S3 Glacier Instant Retrieval
 - low cost object storage meant or archving/ backup
 - pricing: price for storage and object retrieval cost
 - milisecond retrival, great for data accessed once a quarter
 - minimun storage duration of 90 days
 ### Amazon S3 Glacier Flexible Retrieval
  - low cost object storage meant or archving/ backup
  - pricing: price for storage and object retrieval cost
  - expedited (1 to 5 mintues) standard (3 - 5 hours), bulk (5 to 12 hours bulk is free)
  - minimun storage duration of 90 days
 ### Amazon S3 Glacier Deep Archive - long term storage
  - low cost object storage meant or archving/ backup
  - pricing: price for storage and object retrieval cost
  - stardard (12 hours), bulk (48 hours)
  - minimum storage durations of 180 days
 ### Amazon S3 Intelligent Tiering ( able to sit back an relax )
  - small mothly monitoring and auto-teiring fee
  - moves objects automatically between access tiers based on usage
  - there are no retrieval charges
    #### Tiers
     - Frequent Access tier (automatic): default tier
     - Infrequent Access tier (automatic): objects not accessed for 30 days
     - Archive Instant Access tier (automatic): objects not accessed for 90 days
     - Archive Access tier (optional): configurable from 90 days to 700+ days
     - Deep Archive Access tier (optional): config. from 180 days to 700+ days


 

  
    


















